{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## U.S. Bureau of Labor Statistics - CPI Analysis\n",
    "#### Eric Bottinelli"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Retrieve data via BLS API v2\n",
    "\n",
    "**Documentation**\n",
    "\n",
    "- https://www.bls.gov/developers/api_python.htm\n",
    "- https://data.bls.gov/cgi-bin/surveymost?cu\n",
    "- https://data.bls.gov/dataQuery/find?fq=survey:[cu]&s=popularity:D&r=100&st=0\n",
    "- https://www.bls.gov/cpi/tables/relative-importance/2023.htm\n",
    "- https://www.bls.gov/bls/news-release/cpi.htm\n",
    "\n",
    "**Packages to install**\n",
    "\n",
    "- Prettytable ('pip install prettytable')\n",
    "\n",
    "**API Series ID**\n",
    "\n",
    "Consumer Price Index for All Urban Consumers (CPI-U)\n",
    "- *All items in U.S. city average, all urban consumers*\n",
    "    - NSA: CUUR0000SA0\n",
    "    - SA: CUSR0000SA0\n",
    "- *All items less food and energy in U.S. city average, all urban consumers*\n",
    "    - NSA: CUUR0000SA0L1E\n",
    "    - SA: CUSR0000SA0L1E\n",
    "- *Food in U.S. city average, all urban consumers*\n",
    "    - NSA: CUUR0000SAF1\n",
    "    - SA: CUSR0000SAF1\n",
    "- *Food at home in U.S. city average, all urban consumers*\n",
    "    - NSA: CUUR0000SAF11\n",
    "    - SA: CUSR0000SAF11\n",
    "- *Energy in U.S. city average, all urban consumers*\n",
    "    - NSA: CUUR0000SA0E\n",
    "    - SA: CUSR0000SA0E\n",
    "- *Commodities less food and energy commodities in U.S. city average, all urban consumers*\n",
    "    - NSA: CUUR0000SACL1E\n",
    "    - SA: CUSR0000SACL1E\n",
    "- *Services less energy services in U.S. city average, all urban consumers*\n",
    "    - NSA: CUUR0000SASLE\n",
    "    - SA: CUSR0000SASLE\n",
    "- *Shelter in U.S. city average, all urban consumers*\n",
    "    - NSA: CUUR0000SAH1\n",
    "    - SA: CUSR0000SAH1\n",
    "((https://www.bls.gov/cpi/factsheets/owners-equivalent-rent-and-rent.htm))\n",
    "\n",
    "**Calculate special CPI**\n",
    "\n",
    "Occasionally, a user wishes to estimate a price change that is not published by BLS. For instance, suppose a user would like a CPI series for ‘services less energy services and shelter’. This can be done by estimating a special index, in this case, ‘services less energy services and shelter’.\n",
    "[BLS Doc](https://www.bls.gov/cpi/factsheets/constructing-special-cpis.htm)\n",
    "\n",
    "If SEEB01 -> CUUR0000SEEB01\n",
    "\n",
    "Cost weight is just a sum of all the items\n",
    "\n",
    "If I add all the values to calculate the services less energy services and shelter, it becomes a lot of data. Explore different solution (e.g. remove goods from core CPI)\n",
    "\n",
    "**Supercore CPI**\n",
    "\n",
    "\"Fed Chair Jerome Powell cited a specific category of inflation—inflation in core services other than housing—as being perhaps “the most important category for understanding the future evolution of core inflation.” The financial press has termed this category “supercore” inflation\" ([FED of St. Louis](https://www.stlouisfed.org/on-the-economy/2024/may/measuring-inflation-headline-core-supercore-services))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import json\n",
    "import prettytable\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from transformers import BartForConditionalGeneration, BartTokenizer\n",
    "from datetime import datetime\n",
    "\n",
    "# Folder directory containing all the saved data\n",
    "folder_name = 'CPI_Data'\n",
    "if not os.path.exists(folder_name):\n",
    "    os.makedirs(folder_name)\n",
    "\n",
    "# CPI categories weights\n",
    "food_weight = 13.555\n",
    "energy_weight = 6.655\n",
    "services_less_energy_weight = 60.899\n",
    "shelter_weight = 36.191\n",
    "weight_map = {\n",
    "    'All_Items': 100,\n",
    "    'Food_Energy': food_weight + energy_weight,\n",
    "    'Food': food_weight,\n",
    "    'Food_At_Home': 8.167,\n",
    "    'Food_Away_From_Home': 5.388,\n",
    "    'Energy': energy_weight,\n",
    "    'All_Items_Less_Food_Energy': 79.790,\n",
    "    'Commodities_Less_Food_Energy_Commodities': 18.891,\n",
    "    'Services_Less_Energy_Services': services_less_energy_weight,\n",
    "    'Shelter': shelter_weight,\n",
    "    'Supercore': services_less_energy_weight - shelter_weight \n",
    "}\n",
    "\n",
    "# CPI id map\n",
    "series_names = {\n",
    "    'CUUR0000SA0': 'NSA_All_Items',\n",
    "    'CUSR0000SA0': 'SA_All_Items',\n",
    "    'CUUR0000SA0L1E': 'NSA_All_Items_Less_Food_Energy',\n",
    "    'CUSR0000SA0L1E': 'SA_All_Items_Less_Food_Energy',\n",
    "    'CUUR0000SAF': 'NSA_Food',\n",
    "    'CUSR0000SAF': 'SA_Food',\n",
    "    'CUUR0000SAF11': 'NSA_Food_At_Home',\n",
    "    'CUSR0000SAF11': 'SA_Food_At_Home',\n",
    "    'CUUR0000SEFV': 'NSA_Food_Away_From_Home',\n",
    "    'CUSR0000SEFV': 'SA_Food_Away_From_Home',\n",
    "    'CUUR0000SA0E': 'NSA_Energy',\n",
    "    'CUSR0000SA0E': 'SA_Energy',\n",
    "    'CUUR0000SACL1E': 'NSA_Commodities_Less_Food_Energy_Commodities',\n",
    "    'CUSR0000SACL1E': 'SA_Commodities_Less_Food_Energy_Commodities',\n",
    "    'CUUR0000SASLE': 'NSA_Services_Less_Energy_Services',\n",
    "    'CUSR0000SASLE': 'SA_Services_Less_Energy_Services',\n",
    "    'CUUR0000SAH1': 'NSA_Shelter',\n",
    "    'CUSR0000SAH1': 'SA_Shelter',\n",
    "}\n",
    "series_ids = list(series_names.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BLS API request for CPI data\n",
    "\n",
    "Documentation:\n",
    "- https://www.bls.gov/developers/api_signature_v2.htm\n",
    "- Python sample code: https://www.bls.gov/developers/api_python.htm#python2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gather the last 12 months of data\n",
    "current_date = datetime.now()\n",
    "current_year = current_date.year\n",
    "last_year = current_year - 1\n",
    "\n",
    "# API request structure taken from the BLS API documentation and modified to fit the data we need\n",
    "headers = {'Content-type': 'application/json'}\n",
    "data = json.dumps({\"seriesid\": series_ids, \"startyear\": str(last_year), \"endyear\": str(current_year)})\n",
    "response = requests.post('https://api.bls.gov/publicAPI/v2/timeseries/data/', data=data, headers=headers)\n",
    "json_data = json.loads(response.text)\n",
    "\n",
    "all_data = []\n",
    "for series in json_data['Results']['series']:\n",
    "    rows = []\n",
    "    for item in series['data']:\n",
    "        footnotes = \"\".join([footnote['text'] + ',' for footnote in item['footnotes'] if footnote]).rstrip(',')\n",
    "        if 'M01' <= item['period'] <= 'M12':\n",
    "            rows.append([series_names[series['seriesID']], item['year'], item['period'], item['value'], footnotes])\n",
    "\n",
    "    df = pd.DataFrame(rows, columns=[\"series id\", \"year\", \"period\", \"value\", \"footnotes\"])\n",
    "    all_data.append(df)\n",
    "\n",
    "complete_data = pd.concat(all_data)\n",
    "\n",
    "# Process the data to properly save them in a csv file\n",
    "df = complete_data.copy()\n",
    "df['date'] = pd.to_datetime(df['year'].astype(str) + df['period'].str.replace('M', ''), format='%Y%m')\n",
    "df['series id'] = df['series id'].astype(str) \n",
    "df['value'] = pd.to_numeric(df['value'], errors='coerce')\n",
    "df['footnotes'] = df['footnotes'].astype(str) \n",
    "df.drop(['year', 'period', 'footnotes'], axis=1, inplace=True)\n",
    "df.rename(columns={'series id': 'id'}, inplace=True)\n",
    "df = df[['id', 'date', 'value']]\n",
    "\n",
    "csv_path = os.path.join(folder_name, 'CPI_data.csv')\n",
    "df.to_csv(csv_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data processing\n",
    "\n",
    "On top of calculating the MoM and YoY changes, the food + energy and supercore data must be calculated. Furthermore, the data are organized in a table that allows the correct subdivision into categories and sub categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset to avoid making the API request every time\n",
    "df = pd.read_csv(\"CPI_Data/CPI_data.csv\")\n",
    "df['date'] = pd.to_datetime(df['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculation of the MoM and YoY changes\n",
    "df['MoM_change'] = df.groupby('id')['value'].transform(lambda x: (x - x.shift(-1)) / x.shift(-1))\n",
    "df['YoY_change'] = df.groupby('id')['value'].transform(lambda x: (x - x.shift(-12)) / x.shift(-12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_weighted_change(df: pd.DataFrame, id1: str, id2: str, new_id: str, weight1: float, weight2: float, weight_total: float, operation: str) -> pd.DataFrame\n",
    "    '''\n",
    "    Calculate the weighted change of food + energy and supercore for both SA and non-SA (NSA).\n",
    "    \n",
    "    Args:\n",
    "        df: The DataFrame containing the CPI data.\n",
    "        id1: The id of the first series.\n",
    "        id2: The id of the second series.\n",
    "        new_id: The id of the new series.\n",
    "        weight1: The weight of the first series.\n",
    "        weight2: The weight of the second series.\n",
    "        weight_total: The total weight of the new series.\n",
    "        operation: The operation to be performed. Must be 'add' or 'subtract'.\n",
    "    \n",
    "    Returns:\n",
    "        The DataFrame containing the new series\n",
    "    '''\n",
    "    series1 = df.loc[df['id'] == id1, ['MoM_change', 'YoY_change']].set_index(df.loc[df['id'] == id1, 'date']) * weight1\n",
    "    series2 = df.loc[df['id'] == id2, ['MoM_change', 'YoY_change']].set_index(df.loc[df['id'] == id2, 'date']) * weight2\n",
    "\n",
    "    if operation == 'add':\n",
    "        result = (series1 + series2) / weight_total\n",
    "    elif operation == 'subtract':\n",
    "        result = (series1 - series2) / weight_total\n",
    "    else:\n",
    "        raise ValueError(\"Operation must be 'add' or 'subtract'.\")\n",
    "\n",
    "    result = result.reset_index()\n",
    "    result['id'] = new_id\n",
    "    result['value'] = 0\n",
    "    result = result[['id', 'date', 'value', 'MoM_change', 'YoY_change']]\n",
    "    return result\n",
    "\n",
    "# Calculate 'SA_Food_Energy' and 'NSA_Food_Energy'\n",
    "df = pd.concat([\n",
    "    df,\n",
    "    calculate_weighted_change(df, 'SA_Food', 'SA_Energy', 'SA_Food_Energy', weight_map['Food'], weight_map['Energy'], weight_map['Food_Energy'], 'add'),\n",
    "    calculate_weighted_change(df, 'NSA_Food', 'NSA_Energy', 'NSA_Food_Energy', weight_map['Food'], weight_map['Energy'], weight_map['Food_Energy'], 'add')\n",
    "])\n",
    "\n",
    "# Calculate 'SA_Supercore' and 'NSA_Supercore'\n",
    "df = pd.concat([\n",
    "    df,\n",
    "    calculate_weighted_change(df, 'SA_Services_Less_Energy_Services', 'SA_Shelter', 'SA_Supercore', weight_map['Services_Less_Energy_Services'], weight_map['Shelter'], weight_map['Supercore'], 'subtract'),\n",
    "    calculate_weighted_change(df, 'NSA_Services_Less_Energy_Services', 'NSA_Shelter', 'NSA_Supercore', weight_map['Services_Less_Energy_Services'], weight_map['Shelter'], weight_map['Supercore'], 'subtract')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Category mapping to properly organize the data into categories, and give the proper names for the plot\n",
    "category_map = {\n",
    "    'All_Items': (0, 'Headline', '', '', 'All Items'),\n",
    "    'Food_Energy': (1, 'Food + Energy', '', '', 'Food and Energy'),\n",
    "    'Food': (2, '', 'Food', '', 'Food'),\n",
    "    'Food_At_Home': (3, '', '', 'At home', 'Food at Home'),\n",
    "    'Food_Away_From_Home': (4, '', '', 'Away Home', 'Food Away from Home'),\n",
    "    'Energy': (5, '', 'Energy', '', 'Energy'),\n",
    "    'All_Items_Less_Food_Energy': (6, 'Core', '', '', 'Core CPI'),\n",
    "    'Commodities_Less_Food_Energy_Commodities': (7, '', 'Goods', '', 'Goods'),\n",
    "    'Services_Less_Energy_Services': (8, '', 'Services', '', 'Services excluding Energy'),\n",
    "    'Shelter': (9, '', '', 'Shelter', 'Shelter'),\n",
    "    'Supercore': (10, '', '', 'Supercore', 'Supercore')\n",
    "}\n",
    "\n",
    "ordered_categories = ['Headline', 'Food + Energy', 'Core']\n",
    "ordered_sub_categories_1 = ['Food', 'Energy', 'Commodities', 'Services']\n",
    "ordered_sub_categories_2 = ['At home', 'Away Home', 'Shelter', 'Supercore']\n",
    "\n",
    "for i in range(4):\n",
    "    if i in [0, 1]:\n",
    "        data = df[df['id'].str.startswith('NSA_')].copy()\n",
    "        prefix_length = 4\n",
    "    else:\n",
    "        data = df[df['id'].str.startswith('SA_')].copy() \n",
    "        prefix_length = 3\n",
    "\n",
    "    data.loc[:, 'id'] = data['id'].str[prefix_length:]\n",
    "    data.loc[:, 'Month-Year'] = data['date'].dt.strftime('%b-%y')\n",
    "    order_cat = data['id'].apply(lambda x: category_map.get(x, (None, None, None, None, None)))\n",
    "    data.loc[:, 'Order'] = [item[0] for item in order_cat]\n",
    "    data.loc[:, 'Category'] = [item[1] for item in order_cat]\n",
    "    data.loc[:, 'Sub Category 1'] = [item[2] for item in order_cat]\n",
    "    data.loc[:, 'Sub Category 2'] = [item[3] for item in order_cat]\n",
    "    data.loc[:, 'Name'] = [item[4] for item in order_cat]\n",
    "    data.loc[:, 'Weight'] = data['id'].map(weight_map).fillna('Unknown')\n",
    "\n",
    "    if i in [0, 2]:\n",
    "        value_column = 'MoM_change'\n",
    "    else:\n",
    "        value_column = 'YoY_change'\n",
    "\n",
    "    data_pivot = data.pivot_table(\n",
    "        index=['Name', 'Order', 'Category', 'Sub Category 1', 'Sub Category 2', 'Weight'],\n",
    "        columns='Month-Year',\n",
    "        values=value_column,\n",
    "        aggfunc='first'\n",
    "    )\n",
    "\n",
    "    data_pivot = data_pivot[sorted(data_pivot.columns, key=lambda x: pd.to_datetime(x, format='%b-%y'), reverse=True)]\n",
    "    data_pivot.columns.name = None\n",
    "    data_pivot.reset_index(inplace=True)\n",
    "    data_pivot.sort_values(by='Order', inplace=True)\n",
    "    data_pivot.drop(columns=['Order'], inplace=True)\n",
    "\n",
    "    if i == 0:\n",
    "        NSA_MoM_df = data_pivot\n",
    "    elif i == 1:\n",
    "        NSA_YoY_df = data_pivot\n",
    "    elif i == 2:\n",
    "        SA_MoM_df = data_pivot\n",
    "    else:\n",
    "        SA_YoY_df = data_pivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the data\n",
    "csv_path = os.path.join(folder_name, 'NSA_MoM_CPI_data.csv')\n",
    "NSA_MoM_df.to_csv(csv_path, index=False)\n",
    "\n",
    "csv_path = os.path.join(folder_name, 'NSA_YoY_CPI_data.csv')\n",
    "NSA_YoY_df.to_csv(csv_path, index=False)\n",
    "\n",
    "csv_path = os.path.join(folder_name, 'SA_MoM_CPI_data.csv')\n",
    "SA_MoM_df.to_csv(csv_path, index=False)\n",
    "\n",
    "csv_path = os.path.join(folder_name, 'SA_YoY_CPI_data.csv')\n",
    "SA_YoY_df.to_csv(csv_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CPI Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_report_text(url: str) -> str:\n",
    "    '''\n",
    "    The BLS API doesn't provide the report text, so I had to scrape the website to get it. The website structure is simple, so I used BeautifulSoup to get the text. BLS has an anti-scraping system, so I had to add a User-Agent header to the request.\n",
    "\n",
    "    Args:\n",
    "        url: The URL of the webpage to scrape\n",
    "    \n",
    "    Returns:\n",
    "        The text of the report\n",
    "    '''\n",
    "    headers = {\n",
    "        'User-Agent': 'email@domain.name'  # It doesn't matter\n",
    "    }\n",
    "    response = requests.get(url, headers=headers)\n",
    "    if response.status_code == 200: # Code 200 means the request was successful\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        normalnews_div = soup.find('div', class_='normalnews') # After analyzing the webpage using the inspector, I found that the needed text is inside a <div> with class 'normalnews'\n",
    "        if normalnews_div:\n",
    "            return normalnews_div.get_text(separator='\\n', strip=True)\n",
    "        else:\n",
    "            return \"No <div> with class 'normalnews' found.\" # Error management\n",
    "    else:\n",
    "        return f\"Failed to retrieve page: {response.status_code}\"\n",
    "\n",
    "url = \"https://www.bls.gov/news.release/cpi.nr0.htm\"\n",
    "\n",
    "full_text = fetch_report_text(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_paragraph_based_on_index(full_text: str) -> str:\n",
    "    '''\n",
    "    Extract the second paragraph of the main section of the report, or the third paragraph if a \"NOTE:\" is present after the second paragraph (e.g. June 2024). A LLM could be used to find the paragraph of interest, but after the whole process of training, the best result would be for it to understand this pattern. Therefore, I decided to use a simple rule-based approach. For all the previous year, this rule-based approach worked perfectly. A more advanced system that could detect a sudden change in the report structure would be needed to handle the edge cases, but for this project would be overkill.\n",
    "\n",
    "    Args:\n",
    "        full_text: The full text of the report retrieved from the BLS website\n",
    "\n",
    "    Returns:\n",
    "        The paragraph of interest\n",
    "    '''\n",
    "    paragraphs = [p.strip() for p in full_text.replace('\\r', '').split('\\n\\n') if p.strip()] # split the text into paragraphs\n",
    "    for i, paragraph in enumerate(paragraphs):\n",
    "        if \"CONSUMER PRICE INDEX\" in paragraph: # \"CONSUMER PRICE INDEX\" is the section we are interested in, and it's the only consumer price index title written in uppercase\n",
    "            start_index = i\n",
    "            break\n",
    "    else:\n",
    "        return \"CONSUMER PRICE INDEX section not found.\"\n",
    "    note_present = \"NOTE:\" in paragraphs[start_index + 1] # check if a \"NOTE:\" is present immediately after \"CONSUMER PRICE INDEX\"\n",
    "    target_index = start_index + 3 if note_present else start_index + 2\n",
    "    if target_index < len(paragraphs):\n",
    "        target_paragraph = paragraphs[target_index].replace('\\n', ' ') # replace newline characters with a space\n",
    "        return target_paragraph\n",
    "    else:\n",
    "        return \"The paragraph of interest could not be found.\"\n",
    "\n",
    "relevant_paragraph = extract_paragraph_based_on_index(full_text)\n",
    "relevant_paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The index for shelter rose 0.4 percent in July, accounting for nearly 90 percent of the monthly increase in the all items index. The energy index was unchanged over the month, after declining in the two preceding months.\n"
     ]
    }
   ],
   "source": [
    "# I decided to use a BART pre-trained model to summarize the paragraph retrieved above, and ignore the food section of it. The model is fine-tuned for summarization tasks, and it should be able to generate a good summary of the paragraph. The model is not perfect, and the summary could be better, but it should be good enough for this project. I already used BART previously, so I know the capabilities of the model.\n",
    "model_name = \"facebook/bart-large-cnn\" \n",
    "tokenizer = BartTokenizer.from_pretrained(model_name)\n",
    "model = BartForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "input_text = \"Summarize and ignore the food parts: \" + relevant_paragraph # prompt for the model. This could be improved by adding more information to the prompt, but for this project, it should be enough.\n",
    "input_ids = tokenizer.encode(input_text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "\n",
    "# Generate the summary\n",
    "summary_ids = model.generate(\n",
    "    input_ids, \n",
    "    num_beams=4, \n",
    "    min_length=20, \n",
    "    max_length=60,  # Setting a realistic max_length\n",
    "    length_penalty=1.0, \n",
    "    no_repeat_ngram_size=3,  # Prevent repetition of phrases\n",
    "    early_stopping=True  # Allow the model to generate until max_length\n",
    ")\n",
    "summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "# Print the summary\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_path = os.path.join(folder_name, 'summary.txt')\n",
    "\n",
    "# Save the paragraph to a text file\n",
    "with open(txt_path, 'w') as file:\n",
    "    file.write(summary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
